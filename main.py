import os
import json
import logging
from fastapi import FastAPI, Request, Form
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from dotenv import load_dotenv

from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_openai import ChatOpenAI

from hybrid_retriever import HybridRetriever, faiss_retriever, bm25_retriever

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
log = logging.getLogger(__name__)

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI ===
app = FastAPI()
templates = Jinja2Templates(directory=".")

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏–∫–∏ (CSS –∏ HTML –∏–∑ –∫–æ—Ä–Ω—è)
app.mount("/static", StaticFiles(directory=".", html=True), name="static")

# === –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è ===
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY is required")

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM –∏ —Ä–µ—Ç—Ä–∏–≤–µ—Ä–∞ ===
llm = ChatOpenAI(api_key=api_key, model="gpt-4o-mini", temperature=0.7, max_tokens=1000)
hybrid_retriever = HybridRetriever(faiss_retriever, bm25_retriever)

# === –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ===
@app.get("/", response_class=HTMLResponse)
async def serve_ui(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})


# === –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ ===
@app.post("/ask")
async def handle_query(query: str = Form(...)):
    try:
        log.info(f"üì• –ó–∞–ø—Ä–æ—Å: {query}")
        relevant_docs = hybrid_retriever.get_relevant_documents(query)
        log.info(f"üîç –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(relevant_docs)}")

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context = "\n".join(
            f"–í–æ–ø—Ä–æ—Å: {doc.page_content}\n–û—Ç–≤–µ—Ç: {doc.metadata['answer']}"
            for doc in relevant_docs
        )

        # –ü—Ä–æ–º–ø—Ç
        prompt_template = """
–¢—ã –æ–Ω–ª–∞–π–Ω –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ø—Ä–æ–¥—É–∫—Ç–∞–º –∫–æ–º–ø–∞–Ω–∏–∏ –ú–¢–°.
–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º, –≤–µ–∂–ª–∏–≤–æ –∏ –ª—é–±–µ–∑–Ω–æ –æ—Ç–≤–µ—á–∞—è –Ω–∞ –∏—Ö –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–∏.
–û—Ç–≤–µ—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:

{context}

–í–æ–ø—Ä–æ—Å: {query}

–ï—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏ –ø–µ—Ä–µ–π—Ç–∏ –ø–æ —Å—Å—ã–ª–∫–µ "https://support.mts.ru/contacts".
–ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ –ø—Ä–æ–¥—É–∫—Ç–∞–º –ú–¢–° ‚Äî –æ–±—ä—è—Å–Ω–∏, —á—Ç–æ —Ç—ã –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ—à—å —Ç–æ–ª—å–∫–æ –ø–æ –≤–æ–ø—Ä–æ—Å–∞–º –≤–Ω—É—Ç—Ä–∏ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã –ú–¢–°.
–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ.
"""
        prompt = PromptTemplate(input_variables=["context", "query"], template=prompt_template)
        llm_chain = LLMChain(llm=llm, prompt=prompt)
        answer = llm_chain.run({"context": context, "query": query})
        log.info("‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∫–∞–∫ JSON
        relevant_docs_json = [
            {
                "page_content": doc.page_content,
                "file_path": doc.metadata["file_path"]
            }
            for doc in relevant_docs
        ]

        return JSONResponse(content={
            "answer": answer.strip(),
            "relevant_docs": json.dumps(relevant_docs_json, ensure_ascii=False)
        })

    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        return JSONResponse(content={
            "answer": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞.",
            "relevant_docs": "[]"
        })


# === –õ–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)